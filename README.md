# Facial-Emotion-Recognition
In Summer 2022, I did this personal project which is able to predict human emotion by receiving an input image.

**Andrea Miele**

<img src="example_images/demo live.gif" width="400"/>

# Introduction
One of the most important nonverbal forms of communication and social interaction between individuals is facial expression. Because humans are a social creature, we may learn about someone's thoughts or feelings by examining their facial expressions. In cases when an individual is presenting a specific

Through analyzing their reaction to something or their emotional condition or welfare at a specific time, it is possible to discover how they are feeling. The capacity to recognize one's emotion by facial expression is a valuable skill since it may be more instructive than verbal communication in some contexts, particularly when verbal communication is absent or unrevealing of much information.

Machine learning for face emotion recognition is an area prone to mistake owing to fluctuating lighting, location, and other aspects in facial photographs. Furthermore, even by human standards, many people have difficulty recognizing emotion from the face alone. As a result, models will have difficulty obtaining great accuracy.

If a concrete and trustworthy classification model is successfully constructed, it may be used in a variety of sectors. Here are a couple such examples: <br>
- Clinical and psychological applications - Individuals with developmental or mental illnesses, such as autism spectrum disorder and schizophrenia, who frequently difficulty with understanding emotion from facial expressions, might benefit from this service. A dependable model can be utilized to provide instruction to such persons in order to help them develop the abilities required to discern subtleties in facial expressions. In turn, a model can teach appropriate responding emotions, assisting individuals in better understanding particular social circumstances. <br>
- Criminology and forensic psychology - Assisting investigations by recognizing suspects' facial expressions during questioning, similar to the results of a lie detector. <br>
- Human-Computer Interaction (HCI) - Assisting in consumer and market research by assessing ad effectiveness and customer satisfaction. Their results can be enhanced and lead to improved items or recommendations with the use of a dependable model. <br>
- Political Science - Detecting politicians' emotions during public appearances or speeches.

# Methods
## Data Exploration


## Data Preprocessing



## Main model 


# Results


# Discussion


# Conclusion


# Extra-information

# References
## Face Recognition
- [realpython.com](https://realpython.com/face-recognition-with-python/)
- [analyticsvidhya.com](https://www.analyticsvidhya.com/blog/2022/04/face-recognition-system-using-python/)

## Literature Review
[1] Han, K., Yu, D., & Tashev, I. (2014, September). Speech emotion recognition using deep neural
network and extreme learningmachine. Interspeech 2014. <br>
[2] Ng, H. W., Nguyen, V. D., Vonikakis, V., & Winkler, S. (2015, November). Deep learning for emotion
recognition on small datasets using transfer learning. Proceedings of the 2015 ACM on international
conference on multimodal interaction (pp. 443-449). <br>
[3] Minaee, S., Minaei, M., & Abdolrashidi, A. (2021 April). Deep-Emotion: Facial Expression Recognition Using Attentional Convolutional Network. Sensors 2021, 21, 3046.
https://doi.org/10.3390/s21093046 <br>
[4] A. V. Savchenko, L. V. Savchenko and I. Makarov, "Classifying Emotions and Engagement in Online Learning Based on a Single Facial Expression Recognition Neural Network," in IEEE Transactions on Affective Computing, vol. 13, no. 4, pp. 2132-2143, 1 Oct.-Dec. 2022, doi: 10.1109/TAFFC.2022.3188390.<br>
[5] Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., ... & Bengio, Y. (2013, November). Challenges in representation learning: A report on three machine learning contests. In International conference on neural information processing (pp. 117-124). Springer, Berlin, Heidelberg.
